{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afdc0b93-de6f-4286-9a70-91bcf9c9e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142c2ab7-abce-45d5-a36a-cf839bcd7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(906) #906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7f4620-5744-4392-9bbc-6273cb96ac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbfcc72-02b8-447a-a279-46e9a9d8cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM结合了通道注意力和空间注意力，分别关注\"什么\"特征重要和\"哪里\"重要\n",
    " \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool=nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool=nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1=nn.Conv2d(in_planes, in_planes//ratio, 1, bias=False)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.fc2=nn.Conv2d(in_planes//ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        avg_out=self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out=self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out=avg_out+max_out\n",
    "        return self.sigmoid(out)\n",
    " \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        avg_out=torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _=torch.max(x, dim=1, keepdim=True)\n",
    "        x=torch.cat([avg_out, max_out], dim=1)\n",
    "        x=self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    " \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.ca=ChannelAttention(in_planes, ratio)\n",
    "        self.sa=SpatialAttention(kernel_size)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x=x*self.ca(x)\n",
    "        x=x*self.sa(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376e7dbd-1e50-4dd0-830e-e9a5f867251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    M_Num , 代表卷积过滤器（convolutional filters）\n",
    "    N_Num , 代表神经元个数（neurons）\n",
    "    \"\"\"\n",
    "    def __init__(self, M_Num, N_Num, is_se=True):\n",
    "        self.M = M_Num\n",
    "        self.N = N_Num\n",
    "        super().__init__()\n",
    "        self.is_se = is_se\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(136, 64, kernel_size=(4, 8), padding=\"same\"),   \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(4, 8), padding=\"same\"),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(stride=(2, 2), kernel_size=(2, 2)),\n",
    "            nn.Conv2d(128, 64, kernel_size=(4, 8), stride=(1, 1), padding=\"same\"),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),) \n",
    "        if self.is_se:\n",
    "            self.se = CBAM(64) \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(76800, 128), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),  # 缓解过拟合，一定程度上正则化\n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(64, 19),)\n",
    "\n",
    "            \n",
    "    def forward(self, InData):\n",
    "        x = self.conv(InData)\n",
    "        x = self.se(x) * x  \n",
    "        x = nn.Flatten()(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730ac75a-6684-4c3e-8966-3c881388b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformer(nn.Module):\n",
    "    def __init__(self, is_se=True, num_classes=19, num_heads=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvNetwork(96, 96)\n",
    "        # 调整维度以适应 Transformer 输入\n",
    "        self.linear_proj = nn.Linear(76800, 256)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.linear_proj(x)\n",
    "        x = x.unsqueeze(1)  # 添加序列长度维度\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb031d44-adbc-4d72-8ad1-0ea2a232e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvTransformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880fa3e7-960d-4aa1-b717-00e9b00f7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变学习率\n",
    "def train_model(model, DL, valDataL, criterion, optimizer, num_epochs, patience=3):\n",
    "    best_acc = 0.0\n",
    "    # 用于记录验证集准确率没有提升的连续轮数\n",
    "    no_improvement_count = 0\n",
    "    # 创建 ReduceLROnPlateau 调度器，当验证集准确率在 2 个 epoch 内没有提升时，学习率乘以 0.1\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10, factor=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        for batch, (X, y) in enumerate(DL):\n",
    "            X = X.type(torch.FloatTensor)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X)\n",
    "            outputs = outputs.float()\n",
    "            outputs = outputs.to(device)\n",
    "            loss = criterion(outputs, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            preds = outputs.argmax(dim=1)  # 取每个样本的最大值索引\n",
    "            y = y.argmax(dim=1)  # 取每个样本的最大值索引\n",
    "            correct_preds += torch.sum(preds == y).item()\n",
    "            total_preds += y.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(DL)\n",
    "        epoch_acc = correct_preds / total_preds * 100\n",
    "\n",
    "        # 在验证集上评估\n",
    "        model.eval()\n",
    "        val_correct_preds = 0\n",
    "        val_total_preds = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(valDataL):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                preds = outputs.argmax(dim=1)  # 取每个样本的最大值索引\n",
    "                y = y.argmax(dim=1)  # 取每个样本的最大值索引\n",
    "                val_correct_preds += torch.sum(preds == y).item()\n",
    "                val_total_preds += y.size(0)\n",
    "\n",
    "        val_acc = val_correct_preds / val_total_preds * 100\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.2f}%, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # 保存最好的模型\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            # 保存模型\n",
    "            torch.save(model.state_dict(), \"/root/autodl-tmp/model/best_model/cnnbest_model.pth\")\n",
    "            # 重置验证集准确率未提升的计数器\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # 调用调度器更新学习率\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "       \n",
    "\n",
    "    print(f\"训练完成。验证集最高准确率为: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c32b50c-a37d-419e-9676-f46f35c22742",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self,quantile1 = 0.81,quantile2 = 0.31):\n",
    "        super(Loss, self).__init__()\n",
    "        self.quantile1 = quantile1\n",
    "        self.quantile2 = quantile2\n",
    " \n",
    "    def forward(self, y_true, y_pred):\n",
    "        residual = y_pred - y_true\n",
    "        quantileloss = torch.where(y_true > -0.501, torch.max((self.quantile1 - 1) * residual, self.quantile1 * residual), torch.max((self.quantile2 - 1) * residual, self.quantile2 * residual))\n",
    "        quantileloss1 = torch.max((self.quantile1 - 1) * residual, self.quantile1 * residual)\n",
    "        quantileloss2 = torch.max((self.quantile2 - 1) * residual, self.quantile2 * residual)\n",
    "\n",
    "        return torch.mean(quantileloss)\n",
    " \n",
    "loss_fn = Loss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9afafa8-2d26-4307-951c-c37a9a4b118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "train_model(model, DL, valDataL, loss_fn, optimizer, num_epochs=30, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=DataLoader(ENSODataset(type_=\"OBStest\"), batch_size=300, shuffle=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X,y = next(iter(Data))\n",
    "    X = X.type(torch.FloatTensor)\n",
    "    X,y = X.to(device),y.to(device)\n",
    "    # Compute prediction\n",
    "    pred = model(X)\n",
    "    pred = pred.float()\n",
    "    pred = pred.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
